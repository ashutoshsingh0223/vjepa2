{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-JEPA 2 Demo Notebook\n",
    "\n",
    "This tutorial provides an example of how to load the V-JEPA 2 model in vanilla PyTorch and HuggingFace, extract a video embedding, and then predict an action class. For more details about the paper and model weights, please see https://github.com/facebookresearch/vjepa2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_gt = [\n",
    "    '../data/data_sf_1/annotations/task_dji_0011_001.mp4-2023_06_02_12_02_56-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0011_002.mp4-2023_07_24_07_42_53-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0020_001.mp4-2023_07_24_07_07_22-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0020_003.mp4-2023_07_19_11_08_45-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task__dji_0027_001_cut.mp4-2023_03_16_13_41_41-mot 1.1/gt/gt.txt'\n",
    "    '../data/data_sf_1/annotations/task_dji_0069_001.mp4-2023_07_19_12_20_43-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0069_002.mp4-2023_07_19_13_50_09-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0071_001.mp4-2023_07_19_10_50_04-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0071_003.mp4-2023_06_28_09_54_05-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_gh020076.mp4-2023_04_06_13_05_55-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_gh030076.mp4-2023_04_06_11_46_50-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_gh040076.mp4-2023_08_08_14_20_35-mot 1.1/gt/gt.txt',\n",
    "]\n",
    "\n",
    "\n",
    "test_gt = [\n",
    "    '../data/data_sf_1/annotations/task_dji_0011_003.mp4-2023_07_24_08_07_18-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0020_002.mp4-2023_07_19_10_17_29-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0027_002.mp4-2023_07_05_07_48_07-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_dji_0071_002.mp4-2023_06_05_12_42_01-mot 1.1/gt/gt.txt',\n",
    "    '../data/data_sf_1/annotations/task_gh010076.mp4-2023_04_06_12_15_21-mot 1.1/gt/gt.txt',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_videos = [\n",
    "    '../data/data_sf_1/videos/DJI_0011_001.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0011_002.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0020_001.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0020_003.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0027_001_cut.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0069_001.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0069_002.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0071_001.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0071_003.MP4',\n",
    "    '../data/data_sf_1/videos/GH020076.MP4',\n",
    "    '../data/data_sf_1/videos/GH030076.MP4',\n",
    "    '../data/data_sf_1/videos/GH040076.MP4'\n",
    "]\n",
    "\n",
    "testing_videos = [\n",
    "    '../data/data_sf_1/videos/DJI_0011_003.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0020_002.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0027_002.MP4',\n",
    "    '../data/data_sf_1/videos/DJI_0071_002.MP4',\n",
    "    '../data/data_sf_1/videos/GH010076.MP4',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil, sqrt\n",
    "\n",
    "def show_feature_grid(frames, cmap='viridis', figsize=(12, 12), ncols=None):\n",
    "    \"\"\"\n",
    "    Show a grid of scalar feature maps (H, W) as heatmaps.\n",
    "\n",
    "    Args:\n",
    "        frames (np.ndarray): (T, H, W) array of scalar frames.\n",
    "        cmap (str): Colormap for heatmap (e.g., 'viridis', 'plasma').\n",
    "        figsize (tuple): Overall figure size.\n",
    "        ncols (int): Number of columns. If None, auto-calculated.\n",
    "    \"\"\"\n",
    "    T = frames.shape[0]\n",
    "    ncols = ncols or ceil(sqrt(T))\n",
    "    nrows = ceil(T / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes = np.array(axes).reshape(-1)  # Flatten in case of 1D row\n",
    "\n",
    "    vmin = frames.min()\n",
    "    vmax = frames.max()\n",
    "\n",
    "    for i in range(nrows * ncols):\n",
    "        ax = axes[i]\n",
    "        if i < T:\n",
    "            ax.imshow(frames[i], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f\"Frame {i}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def interpolate_temporal_features_to_frame_space(features, patch_size=16, output_T=64, method='norm'):\n",
    "    \"\"\"\n",
    "    Interpolates ViT patch features (temporal + spatial) to original frame space.\n",
    "\n",
    "    Args:\n",
    "        features (np.ndarray): shape (T, H_p, W_p, C)\n",
    "        patch_size (int): Patch size used in encoder (default 16).\n",
    "        output_T (int): Target number of frames (e.g., 64).\n",
    "        method (str): 'mean', 'norm', or 'first' for reducing channels.\n",
    "        visualize (bool): Whether to show selected frames.\n",
    "\n",
    "    Returns:\n",
    "        upsampled_features (np.ndarray): (output_T, H, W), per-frame scalar maps.\n",
    "    \"\"\"\n",
    "    T, H_p, W_p, C = features.shape\n",
    "    features_torch = torch.from_numpy(features).permute(0, 3, 1, 2).float()  # (T, C, H_p, W_p)\n",
    "\n",
    "    # Collapse channels to 1\n",
    "    if method == \"mean\":\n",
    "        features_collapsed = features_torch.mean(dim=1, keepdim=True)  # (T, 1, H_p, W_p)\n",
    "    elif method == \"norm\":\n",
    "        features_collapsed = torch.norm(features_torch, dim=1, keepdim=True)  # (T, 1, H_p, W_p)\n",
    "    elif method == \"first\":\n",
    "        features_collapsed = features_torch[:, 0:1, :, :]  # (T, 1, H_p, W_p)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'mean', 'norm', or 'first'\")\n",
    "\n",
    "    # Upsample spatially\n",
    "    H = H_p * patch_size\n",
    "    W = W_p * patch_size\n",
    "    features_spatial = F.interpolate(\n",
    "        features_collapsed.permute(1, 0, 2, 3),  # (1, T, H_p, W_p)\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze(0)  # (T, H, W)\n",
    "\n",
    "    # Now interpolate temporally\n",
    "    features_spatial = features_spatial.permute(1, 2, 0)  # (H, W, T)\n",
    "    features_spatial = features_spatial.reshape(-1, T).unsqueeze(1)  # (H*W, 1, T)\n",
    "    features_interp = F.interpolate(features_spatial, size=output_T, mode=\"linear\", align_corners=False)  # (H*W, 1, output_T)\n",
    "\n",
    "    # Reshape back to (output_T, H, W)\n",
    "    features_interp = features_interp.squeeze(1).reshape(H, W, output_T).permute(2, 0, 1).numpy()  # (output_T, H, W)\n",
    "    return features_interp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def visualize_frame_grid(frames, grid_rows=None, grid_cols=None, figsize=(12, 12), title=\"Frame Grid\"):\n",
    "    \"\"\"\n",
    "    Visualize a stack of frames (as a numpy array) in a grid.\n",
    "\n",
    "    Args:\n",
    "        frames (np.ndarray): Array of shape (N, H, W, 3)\n",
    "        grid_rows (int): Number of rows in the grid. If None, auto-calculated.\n",
    "        grid_cols (int): Number of cols in the grid. If None, auto-calculated.\n",
    "        figsize (tuple): Size of the matplotlib figure.\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    N, H, W, C = frames.shape\n",
    "    assert C == 3, \"Frames must be RGB (HxWx3)\"\n",
    "    \n",
    "    if grid_rows is None or grid_cols is None:\n",
    "        grid_cols = int(math.sqrt(N))\n",
    "        grid_rows = math.ceil(N / grid_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=figsize)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    for i in range(grid_rows * grid_cols):\n",
    "        row = i // grid_cols\n",
    "        col = i % grid_cols\n",
    "        ax = axes[row, col] if grid_rows > 1 else axes[col]\n",
    "        ax.axis('off')\n",
    "\n",
    "        if i < N:\n",
    "            frame = frames[i]\n",
    "            ax.imshow(frame.astype(np.uint8))\n",
    "        else:\n",
    "            ax.set_visible(False)  # Hide empty subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the necessary libraries and load the necessary functions for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from decord import VideoReader\n",
    "from transformers import AutoVideoProcessor, AutoModel\n",
    "\n",
    "import src.datasets.utils.video.transforms as video_transforms\n",
    "import src.datasets.utils.video.volume_transforms as volume_transforms\n",
    "from src.models.attentive_pooler import AttentiveClassifier, AttentivePooler\n",
    "from src.models.vision_transformer import vit_giant_xformers_rope, vit_huge_rope\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def load_pretrained_vjepa_pt_weights(model, pretrained_weights):\n",
    "    # Load weights of the VJEPA2 encoder\n",
    "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
    "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"encoder\"]\n",
    "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    pretrained_dict = {k.replace(\"backbone.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
    "\n",
    "\n",
    "def load_pretrained_vjepa_classifier_weights(model, pretrained_weights):\n",
    "    # Load weights of the VJEPA2 classifier\n",
    "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
    "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
    "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
    "\n",
    "\n",
    "def build_pt_video_transform(img_size):\n",
    "    short_side_size = int(256.0 / 224 * img_size)\n",
    "    # Eval transform has no random cropping nor flip\n",
    "    eval_transform = video_transforms.Compose(\n",
    "        [\n",
    "            video_transforms.Resize(short_side_size, interpolation=\"bilinear\"),\n",
    "            # video_transforms.CenterCrop(size=(img_size, img_size)),\n",
    "            volume_transforms.ClipToTensor(),\n",
    "            video_transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "        ]\n",
    "    )\n",
    "    return eval_transform\n",
    "\n",
    "\n",
    "def get_video():\n",
    "    vr = VideoReader(\"sample_video.mp4\")\n",
    "    # choosing some frames here, you can define more complex sampling strategy\n",
    "    frame_idx = np.arange(0, 128, 2)\n",
    "    video = vr.get_batch(frame_idx).asnumpy()\n",
    "    return video\n",
    "\n",
    "\n",
    "def get_video_path(video_path):\n",
    "    vr = VideoReader(video_path)\n",
    "    # choosing some frames here, you can define more complex sampling strategy\n",
    "    frame_idx = np.arange(30*360, (30*360) + 128, 2)\n",
    "    video = vr.get_batch(frame_idx).asnumpy()\n",
    "    return video\n",
    "\n",
    "\n",
    "def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform):\n",
    "    # Run a sample inference with VJEPA\n",
    "    with torch.inference_mode():\n",
    "        # Read and pre-process the image\n",
    "        video = get_video()  # T x H x W x C\n",
    "        video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
    "        x_pt = pt_transform(video).cuda().unsqueeze(0)\n",
    "        x_hf = hf_transform(video, return_tensors=\"pt\")[\"pixel_values_videos\"].to(\"cuda\")\n",
    "        # Extract the patch-wise features from the last layer\n",
    "        out_patch_features_pt = model_pt(x_pt)\n",
    "        out_patch_features_hf = model_hf.get_vision_features(x_hf)\n",
    "\n",
    "    return out_patch_features_hf, out_patch_features_pt\n",
    "\n",
    "\n",
    "def forward_vjepa_video_pt(model_pt, pt_transform, video):\n",
    "    # Run a sample inference with VJEPA\n",
    "    with torch.inference_mode():\n",
    "        # video -> # T x H x W x C\n",
    "        video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
    "        x_pt = pt_transform(video).cuda().unsqueeze(0)\n",
    "        # Extract the patch-wise features from the last layer\n",
    "        out_patch_features_pt = model_pt(x_pt)\n",
    "\n",
    "    return out_patch_features_pt\n",
    "\n",
    "\n",
    "def get_vjepa_video_classification_results(classifier, out_patch_features_pt):\n",
    "    SOMETHING_SOMETHING_V2_CLASSES = json.load(open(\"ssv2_classes.json\", \"r\"))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out_classifier = classifier(out_patch_features_pt)\n",
    "\n",
    "    print(f\"Classifier output shape: {out_classifier.shape}\")\n",
    "\n",
    "    print(\"Top 5 predicted class names:\")\n",
    "    top5_indices = out_classifier.topk(5).indices[0]\n",
    "    top5_probs = F.softmax(out_classifier.topk(5).values[0]) * 100.0  # convert to percentage\n",
    "    for idx, prob in zip(top5_indices, top5_probs):\n",
    "        str_idx = str(idx.item())\n",
    "        print(f\"{SOMETHING_SOMETHING_V2_CLASSES[str_idx]} ({prob}%)\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import decord\n",
    "from decord import VideoReader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiVideoClipWithMOTDataset(Dataset):\n",
    "    def __init__(self, video_paths, mot_paths, clip_len=64, frame_rate=5, original_fps=30):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            video_paths (list): List of video file paths.\n",
    "            mot_paths (list): List of corresponding MOT .txt files.\n",
    "        \"\"\"\n",
    "        assert len(video_paths) == len(mot_paths), \"Mismatch in number of videos and MOT files.\"\n",
    "\n",
    "        self.clip_len = clip_len\n",
    "        self.frame_rate = frame_rate\n",
    "        self.step = original_fps // frame_rate\n",
    "\n",
    "        self.video_readers = []\n",
    "        self.mot_annotations = []\n",
    "        self.index_map = []  # global_idx -> (video_idx, clip_start)\n",
    "\n",
    "        for vid_idx, (vpath, mpath) in enumerate(zip(video_paths, mot_paths)):\n",
    "            vr = VideoReader(vpath)\n",
    "            total_frames = len(vr)\n",
    "            mot_data = self._load_mot(mpath)\n",
    "\n",
    "            valid_starts = self._compute_valid_clips(total_frames, mot_data)\n",
    "\n",
    "            for start_idx in valid_starts:\n",
    "                self.index_map.append((vid_idx, start_idx))\n",
    "\n",
    "            self.video_readers.append(vr)\n",
    "            self.mot_annotations.append(mot_data)\n",
    "\n",
    "    def _load_mot(self, mot_path):\n",
    "        mot = {}\n",
    "        with open(mot_path, 'r') as f:\n",
    "            for line in f:\n",
    "                items = line.strip().split(',')\n",
    "                frame_id = int(items[0])\n",
    "                obj_id = int(items[1])\n",
    "                if frame_id not in mot:\n",
    "                    mot[frame_id] = set()\n",
    "                mot[frame_id].add(obj_id)\n",
    "        return mot\n",
    "\n",
    "    def _compute_valid_clips(self, total_frames, mot_data):\n",
    "        valid_indices = []\n",
    "        max_start = total_frames - self.clip_len * self.step\n",
    "        for start in range(0, max_start + 1, self.step):\n",
    "            clip_indices = [start + i * self.step for i in range(self.clip_len)]\n",
    "            if any((f + 1) in mot_data for f in clip_indices):  # MOT uses 1-based frame indices\n",
    "                valid_indices.append(start)\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_idx, start = self.index_map[idx]\n",
    "        vr = self.video_readers[video_idx]\n",
    "        mot_data = self.mot_annotations[video_idx]\n",
    "\n",
    "        frame_indices = [start + i * self.step for i in range(self.clip_len)]\n",
    "        frames = vr.get_batch(frame_indices).asnumpy()  # (64, H, W, 3)\n",
    "\n",
    "        unique_ids = set()\n",
    "        for f in frame_indices:\n",
    "            obj_ids = mot_data.get(f + 1, [])  # +1 for 1-based MOT\n",
    "            unique_ids.update(obj_ids)\n",
    "\n",
    "        return frames, len(unique_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveRegressor(nn.Module):\n",
    "    \"\"\"Attentive Classifier\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=768,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        depth=1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        init_std=0.02,\n",
    "        qkv_bias=True,\n",
    "        complete_block=True,\n",
    "        use_activation_checkpointing=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pooler = AttentivePooler(\n",
    "            num_queries=1,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            depth=depth,\n",
    "            norm_layer=norm_layer,\n",
    "            init_std=init_std,\n",
    "            qkv_bias=qkv_bias,\n",
    "            complete_block=complete_block,\n",
    "            use_activation_checkpointing=use_activation_checkpointing,\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.backbone.embed_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pooler(x).squeeze(1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = get_video_path(\"../data_sf_1/videos/DJI_0011_002.MP4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frame_grid(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's download a sample video to the local repository. If the video is already downloaded, the code will skip this step. Likewise, let's download a mapping for the action recognition classes used in Something-Something V2, so we can interpret the predicted action class from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video_path = \"sample_video.mp4\"\n",
    "# Download the video if not yet downloaded to local path\n",
    "if not os.path.exists(sample_video_path):\n",
    "    video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4\"\n",
    "    command = [\"wget\", video_url, \"-O\", sample_video_path]\n",
    "    subprocess.run(command)\n",
    "    print(\"Downloading video\")\n",
    "\n",
    "# Download SSV2 classes if not already present\n",
    "ssv2_classes_path = \"ssv2_classes.json\"\n",
    "if not os.path.exists(ssv2_classes_path):\n",
    "    command = [\n",
    "        \"wget\",\n",
    "        \"https://huggingface.co/datasets/huggingface/label-files/resolve/d79675f2d50a7b1ecf98923d42c30526a51818e2/\"\n",
    "        \"something-something-v2-id2label.json\",\n",
    "        \"-O\",\n",
    "        \"ssv2_classes.json\",\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    print(\"Downloading SSV2 classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/vjepa2/vith.pt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the models in both vanilla Pytorch as well as through the HuggingFace API. Note that HuggingFace API will automatically load the weights through `from_pretrained()`, so there is no additional download required for HuggingFace.\n",
    "\n",
    "To download the PyTorch model weights, use wget and specify your preferred target path. See the README for the model weight URLs.\n",
    "E.g. \n",
    "```\n",
    "wget https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt -P YOUR_DIR\n",
    "```\n",
    "Then update `pt_model_path` with `YOUR_DIR/vitg-384.pt`. Also note that you have the option to use `torch.hub.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace model repo name\n",
    "hf_model_name = (\n",
    "    \"facebook/vjepa2-vitg-fpc64-384\"  # Replace with your favored model, e.g. facebook/vjepa2-vitg-fpc64-384\n",
    ")\n",
    "# Path to local PyTorch weights\n",
    "pt_model_path = \"./vith.pt\"\n",
    "\n",
    "# Initialize the HuggingFace model, load pretrained weights\n",
    "# model_hf = AutoModel.from_pretrained(hf_model_name)\n",
    "# model_hf.cuda().eval()\n",
    "\n",
    "# Build HuggingFace preprocessing transform\n",
    "# hf_transform = AutoVideoProcessor.from_pretrained(hf_model_name)\n",
    "# img_size = hf_transform.crop_size[\"height\"]  # E.g. 384, 256, etc.\n",
    "\n",
    "# Initialize the PyTorch model, load pretrained weights\n",
    "img_size = 256\n",
    "model_pt = vit_huge_rope(img_size=(img_size, img_size), num_frames=64)\n",
    "model_pt.cuda().eval()\n",
    "load_pretrained_vjepa_pt_weights(model_pt, pt_model_path)\n",
    "\n",
    "### Can also use torch.hub to load the model\n",
    "# model_pt, _ = torch.hub.load('facebookresearch/vjepa2', 'vjepa2_vit_giant_384')\n",
    "# model_pt.cuda().eval()\n",
    "\n",
    "# Build PyTorch preprocessing transform\n",
    "pt_video_transform = build_pt_video_transform(img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = forward_vjepa_video_pt(model_pt, pt_video_transform, video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.squeeze(0)\n",
    "reshaped = features.reshape((64 // model_pt.tubelet_size, 292 // 16, 519 // 16, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_grid(interpolate_temporal_features_to_frame_space(reshaped.cpu().numpy(), patch_size=16, output_T=64, method=\"norm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the encoder on the video to get the patch-wise features from the last layer of the encoder. To verify that the HuggingFace and PyTorch models are equivalent, we will compare the values of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on video to get the patch-wise features\n",
    "out_patch_features_hf, out_patch_features_pt = forward_vjepa_video(\n",
    "    model_hf, model_pt, hf_transform, pt_video_transform\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Inference results on video:\n",
    "    HuggingFace output shape: {out_patch_features_hf.shape}\n",
    "    PyTorch output shape:     {out_patch_features_pt.shape}\n",
    "    Absolute difference sum:  {torch.abs(out_patch_features_pt - out_patch_features_hf).sum():.6f}\n",
    "    Close: {torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3)}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we know that the features from both models are equivalent. Now let's run a pretrained attentive probe classifier on top of the extracted features, to predict an action class for the video. Let's use the Something-Something V2 probe. Note that the repository also includes attentive probe weights for other evaluations such as EPIC-KITCHENS-100 and Diving48.\n",
    "\n",
    "To download the attentive probe weights, use wget and specify your preferred target path. E.g. `wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P YOUR_DIR`\n",
    "\n",
    "Then update `classifier_model_path` with `YOUR_DIR/ssv2-vitg-384-64x2x3.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "classifier_model_path = \"YOUR_ATTENTIVE_PROBE_PATH\"\n",
    "classifier = (\n",
    "    AttentiveClassifier(embed_dim=model_pt.embed_dim, num_heads=16, depth=4, num_classes=174).cuda().eval()\n",
    ")\n",
    "load_pretrained_vjepa_classifier_weights(classifier, classifier_model_path)\n",
    "\n",
    "# Get classification results\n",
    "get_vjepa_video_classification_results(classifier, out_patch_features_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video features a man putting a bowling ball into a tube, so the predicted action of \"Putting [something] into [something]\" makes sense!\n",
    "\n",
    "This concludes the tutorial. Please see the README and paper for full details on the capabilities of V-JEPA 2 :)"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "f0b70ba6-1c84-47e1-81bd-b7642f9acf50",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
